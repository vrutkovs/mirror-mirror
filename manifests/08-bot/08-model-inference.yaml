apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    openshift.io/display-name: mirror-mirror
    security.opendatahub.io/enable-auth: "true"
    serving.knative.openshift.io/enablePassthrough: "true"
    serving.kserve.io/deploymentMode: Serverless
    sidecar.istio.io/inject: "true"
    sidecar.istio.io/rewriteAppHTTPProbers: "true"
  name: mirror-mirror
  namespace: mirror-mirror
  labels:
    opendatahub.io/dashboard: "true"
spec:
  predictor:
    annotations:
      serving.knative.dev/progress-deadline: 30m
    maxReplicas: 1
    minReplicas: 1
    containers:
      - name: kserve-container
        image: "image-registry.openshift-image-registry.svc:5000/mirror-mirror/mirror-mirror:model"
        resources:
          limits:
            cpu: "10"
            memory: 30Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: "6"
            memory: 24Gi
            nvidia.com/gpu: "1"
        env:
          - name: MODEL_ID
            value: "stabilityai/stable-diffusion-3-medium-diffusers"
          - name: HF_TOKEN
            value: "foobar"
        volumeMounts:
          - mountPath: /opt/app-root/src/.cache
            name: hg-cache
    tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Exists
    volumes:
      - persistentVolumeClaim:
          claimName: hg-cache
        name: hg-cache
