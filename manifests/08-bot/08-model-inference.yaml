apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    openshift.io/display-name: mirror-mirror
    security.opendatahub.io/enable-auth: "true"
    serving.knative.openshift.io/enablePassthrough: "true"
    serving.kserve.io/deploymentMode: Serverless
    sidecar.istio.io/inject: "true"
    sidecar.istio.io/rewriteAppHTTPProbers: "true"
  name: mirror-mirror
  namespace: mirror-mirror
  labels:
    opendatahub.io/dashboard: "true"
spec:
  predictor:
    annotations:
      serving.knative.dev/progress-deadline: 30m
    maxReplicas: 1
    minReplicas: 1
    containers:
      - name: kserve-container
        image: "image-registry.openshift-image-registry.svc:5000/mirror-mirror/mirror-mirror:model"
        command:
          - bash
          - "-c"
          - "huggingface-cli download ${MODEL_ID} && \
            huggingface-cli download ${LORA_MODEL} && \
            source /app/.venv/bin/activate && \
            exec python /app/model.py"
        resources:
          limits:
            cpu: "8"
            memory: 30Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: "1"
            memory: 8Gi
            nvidia.com/gpu: "1"
        envFrom:
          - secretRef:
              name: settings
        volumeMounts:
          - mountPath: /opt/app-root/src/.cache
            name: hg-cache
    tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Exists
    volumes:
      - persistentVolumeClaim:
          claimName: hg-cache
        name: hg-cache
      - name: settings
        secret:
          secretName: settings
          defaultMode: 420
